---
title: "Scaling the Stars"
subtitle: "Optimizing MPI communication on GPUs in the PROMPI stellar dynamics code"
date: "2025-05-19"
date-format: "dddd DD MMMM YYYY"
format:
  clean-revealjs:
    embed-resources: true
    transition: slide
    logo: media/dirac_logo.png
    footer-logo-link: https://dirac.ac.uk/
    footer: HPC RSE SIG meeting
    header-logo: https://www.cam.ac.uk/sites/all/themes/fresh/images/interface/cambridge_university2.svg
    header-logo-link: https://www.hpc.cam.ac.uk/
    sc-sb-title: true
    menu:
      openButton: false

filters:
 - reveal-header

authors:
 - name: Miren Radia
   role: Research Software Engineer
   affiliations:
     - name: Research Computing Services, University of Cambridge

---
# Introduction {background-color="#40666e"}

## The team {.smaller}

:::: {.columns}

::: {.column width="21.25%"}
![**Raphael Hirschi**</br>
[Professor of Stellar Hydrodynamics and Nuclear Astrophysics]{.alert}</br>
*Keele University*](media/raphael.jpg){width="100%"}
:::

::: {.column width="5%"}
:::

::: {.column width="21.25%"}
![**Vishnu Varma**</br>
[Research Associate In Theoretical Stellar Astrophysics]{.alert}</br>
*Keele University*](media/vishnu.jpg){width="100%"}
:::

::: {.column width="5%"}
:::

::: {.column width="21.25%"}
![**Kate Goodman**</br>
[PhD Student]{.alert}</br>
*Keele University*](media/kate.jpg){width="100%"}
:::

::: {.column width="5%"}
:::

::: {.column width="21.25%"}
::: {.r-stack}
::: {.fragment .fade-out fragment-index=1}
![**Miren Radia**</br>
[Research Software Engineer]{.alert}</br>
*University of Cambridge*](media/miren.jpg){width="100%"}
:::

::: {.fragment .fade-in fragment-index=1}
![**Simon Clifford**</br>
[Research Software Engineer]{.alert}</br>
*University of Cambridge*](media/simon.jpg){width="100%"}
:::
:::
:::

::::

#### Others

* Federico Rizzuti, former PhD Student, *Keele University*

## PROMPI {.smaller}
### What does the code do?

:::: {.columns}

::: {.column width="50%"}
* PROMPI is a fluid dynamics code that is used to simulate complex hydrodynamic
  processes within stars.
* Numerical methods:
  * Finite volume
  * Eulerian
  * Piecewise Parabolic Method (PPM) hydrodynamics scheme
* Physics:
  * Fully compressible fluids
  * Nuclear burning
  * Convection/turbulence
* Code:
  * Fortran
  * Parallelised with domain decomposition distributed with MPI
:::

::: {.column width="50%"}
<!-- {{< video vmag_vhrez.mp4 width=540 height=540 >}} -->
![Evolution of $|\mathbf{v}|$ for a $1024^3$ simulation of the Carbon-burning
shell](media/vmag_vhrez.mp4)
:::

::::

## Previous RSE work
### What improvements had already been made to the code?

Over several DiRAC RSE projects, the code has been enhanced and modernized in
several different ways:

::: {.incremental}
* Acceleration on Nvidia GPUs using OpenACC
* Fortran 77 → Modern free-form Fortran
* Object-oriented design (Fortran 2003)
* Legacy include statements and common blocks → Modules
* Custom Makefile build system → CMake
* Custom binary I/O format → HDF5
* Regression tests and GitLab CI pipeline to run them
:::

# This project {background-color="#40666e"}

## Aims
### What still needed to be done for the new code to be research-ready?

Despite the enhancements, there was still work that needed to be done before the
group felt they could switch over:

::: {.incremental}
1. Consistency between the results on GPU and CPU.
1. Optimal performance on the DiRAC systems the group uses (COSMA8 and Tursa).
1. Porting and testing of physics modules and initial conditions to simulate
   specific scenarios from the old version of the code.
1. Poor scaling on GPUs beyond a single GPU.
:::

## Work summary
### What improvements were made to the code?

During the project, changes I made include:

::: {.incremental}
* Improvements and updates to the CMake build system.
* Dependency software stack creation on Tursa and greenHPC (Keele local system).
* Refactoring, updating and adding to the test and CI frameworks.
* Fixing and refactoring the analysis/plotting Python scripts.
* [Significant refactoring of the MPI communication.]{.fragment .highlight-red}
* Fixing the HDF5 checkpoint and restart consistency.
* [Benchmarking and scaling analysis.]{.fragment .highlight-red}
:::

# Improving MPI communication {background-color="#40666e"}

## The problem {.smaller}
### What was causing such poor performance on GPUs?

::: {.fragment}
Previously the code used:

* Nvidia managed memory extension to OpenACC:
  * The runtime automatically migrates data between host (CPU) and device
    (GPU) as required.
* MPI derived datatypes:
  * `MPI_Type_vector` to simplify halo/ghost cell exchange since this data is
     non-contiguous in memory, albeit regularly spaced:
  ![Non-contiguous memory layout in an
      `MPI_Type_vector`](media/mpi_type_vector_memory_layout.svg)
* Effectively blocking MPI calls:
  * The ghosts for each variable were all sent in separate `MPI_Isend`s.
  * However, `MPI_Wait` was called after every `MPI_Irecv`.
:::

::: {.fragment}
This combination meant lots of small host-device data migrations → bad for
performance:

* For a $512^3$ test simulation running on 8 Tursa Nvidia A100s (2 nodes), > 90%
  of the walltime was spent in communication.
:::

## The solution[^1] {.smaller}
### How was it sped up?

::: {.fragment}
I significantly refactored the communication in the following ways:

* Manual packing and unpacking of data:
  * No more `MPI_Type_vector`.
  * Single send/receive buffer for each pair of communicating processes.
    (data from all variables).
  * Uses asynchronous OpenACC kernels to [un]pack data on the GPU.
* Forced use of GPU-aware MPI:
  ```{.fortran}
  !$acc host_data use_device(send_buf)
  call mpi_isend(send_buf, ...)
  ```
* `MPI_Waitall` after all sends and receives for each direction.
:::

::: {.fragment}
After these changes with our $512^3$ test case on 8 Tursa Nvidia A100s:

* ~200x speed-up in communication leading to ~20x overall speed-up.
* < 10% of the walltime spent in communication.
:::

[^1]: I would like to acknowledge Filippo Spiga (Nvidia) for his help with this work.

# Benefits {background-color="#40666e"}

## Scaling {.smaller}
### How does PROMPI perform after these improvements?

:::: {.columns}

::: {.column width=35%}
#### Weak scaling on Tursa
* Excellent weak scaling of 88% efficiency up to 128 GPUs.
* Most relevant scaling for group given typical research workflows.
![](media/weak_scaling.png)
:::

::: {.column width=30%}
#### Strong scaling on Tursa and COSMA8
* Good strong scaling (>50% efficiency) up to around 32 Tursa Nvidia A100 80GB GPUs.
* Efficiency drops for greater numbers due to GPU underutilization.
* Grey line shows roughly how many COSMA8 (Milan) nodes are equivalent to 1 Tursa GPU.
:::

::: {.column width=35%}
![](media/strong_scaling.png)
:::

::::

## Other benefits {.smaller}
### What else has been achieved as a result of this project?

Performance and scaling improvements were not the only outcomes of this project.
Other benefits include:

::: {.fragment}
* Sustainability and maintainability improvements from:
  * Better CI infrastructure.
  * Additional tests and increased robustness.
:::
::: {.fragment}
* HPC system support from:
  * Improvements to CMake including support for newer compilers.
  * Software dependencies built on Tursa and local HPC.
:::
::: {.fragment}
* Capabilities:
  * Better reproducibility from HDF5 checkpoint improvements.
  * More useful and accurate visualizations from plotting script improvements.
:::

# Any questions? {background-color="#40666e"}